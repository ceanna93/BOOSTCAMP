조원님이 모두에게:  05:58 PM
중학생한테 경사하강법 설명하기

하산

백진우님이 모두에게:  05:58 PM
Attention mechanism을 중심으로 Transformer 구조에 대해 간략히 설명해 주세요

https://wikidocs.net/31379
Encoder단에 하나, Decoder 단에 두 개의 Attention이 사용

이태환님이 모두에게:  05:59 PM
LSTM을 단일 layer로만 사용해야하는지? 여러층으로 사용해도 된다면 그 이유는 무엇인지? 설명해주세요

stacked LSTM
Long Term Dependency 문제가 상대적으로 적은 비디오와 같은 분야에서 사용
실제로 성능은 일반 LSTM에 비해 좋다.

조호성님이 모두에게:  05:59 PM
DataLoader에서 num_workers 파라미터에 대해 설명해주세요

num_workers은 학습 도중 CPU의 작업을 몇 개의 코어를 사용해서 진행할지에 대한 설정 파라미터

문 재훈님이 모두에게:  05:59 PM
1.Transformer의 구조를 attention을 기반으로 설명해주세요. ( Attention 3번을 각각 설명해보기 )

2.딥러닝 가중치 초기값이 0이라면 어떤 문제가 생길까요?

3.LSTM 단을 여러 층으로 쌓을 수 있을까, 그리고 성능이 좋을까?

4.DataLoaer에서 num_workers는 어떤 역할을 하는 파라미터인가?

5.어느 설명변수와 반응변수 간의 상관계수가 0에 가까운 값이었지만, 모델 예측에 있어서 유의한 변수로 나타날 수 있습니다. 어떻게 이런 경우가 발생할까요?

6.경사하강법을 초중생에게 가르쳐야 한다면 뭐라고 설명하겠는가?

나로부터 모두에게:  05:59 PM
초기 가중치가 전부 0일 때 학습이 정상적으로 되는지

만약 데이터를 평균 0정도로 정규화시킨다면, 가중치를 0으로 초기화 시킨다는 생각은 꽤 합리적으로 보일 수 있다. 그러나 실제로 0으로 가중치를 초기화 한다면 모든 뉴런들이 같은 값을 나타낼 것이고, 역전파 과정에서 각 가중치의 update가 동일하게 이뤄질 것이다. 이러한 update는 학습을 진행 해도 계속해서 발생할 것이며, 결국 제대로 학습하기 어려울 것이다. 또한 이러한 동일한 update는 여러 층으로 나누는 의미를 상쇄시킨다.
https://reniew.github.io/13/

0을 가지게 되면 입력 값의 데이터가 무시
https://copycode.tistory.com/173
