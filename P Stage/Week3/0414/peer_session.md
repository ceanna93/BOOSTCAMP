4/14
augmentation masking을 늘리거나 비슷한 단어로 대체
entity를 다른 이름으로 바꾼다.
유의어로 단어를 교체
-> 유의어에 대한 데이터베이스가 필요
random으로 삽입 삭제
다른 나라 말로 번역했다가 한국말로 다시 번역하는 경우 의미는 유지되지만 단어가 달라진다.


류재희님 공유
BERT에서는 전체 token의 15%를 masking하는데 그 중 또 일부는 random word로 만들고 또 일부는 그대로 둔다는 것의 의미는 비유를 하자면 저희가 시험 공부를 하는데 예를 들어 무슨 영어 단어 빈칸 채우기를 한다고 해봅시다. 일단 처음엔 뭘 모르니까 아무 단어나 넣어보고 답지를 보고 확인을 하겠죠. 그렇게 점점 전반적인 semantic한 information을 익히게 되구요. 근데 BERT의 pretraining에선 이렇게 masking한 단어는 학습이 끝날 때까지 쭉 계속 masking된 상태입니다. 즉, 비록 우리가 loss를 통해 학습을 하기는 하지만, 실제 정답 단어를 직접적으로 접하지 못하고 간접적으로 "너 이거 정답 아니야"라고 말하는 것만 계속 듣게 되면서 배우게 되는겁니다. 결국 언젠간 이 단어에 대해 알게 되겠지만 상대적으로 다른 단어에 비해 내가 온전히 접하지 못하고 그냥 계속 "내가 맞추려고 했던 어떤 단어 무언가"정도로만 기억하고 넘어가버리게 되는겁니다. 이러면 나중에 fine-tuning을 하고 test를 할 때 이 단어에 대해선 상대적으로 이해도가 떨어지게 되는거죠. 그런 리스크를 줄여주기 위해 아주 다 masking하는 건 조금 위험하고 적당히 random하게 몇몇 단어는 열어주자는 취지였다고 보시면 됩니다.
10% of the time, keep the sentence as same : 약 1.5%는 원래 단어로 그대로 둔 채, 해당 단어가 혹시 다른 단어로 바뀌어야 하는지, 그 해당 단어에 대한 토큰을 가지고 인코딩 벡터를 아웃풋 레이어를 통해 예측 했을 때, 이 경우 원래 있었던 단어와 동일해야 한다고 소신 있게 예측할 수 있는 형태의 학습도 유도



버트 같은 timeseq가 무관한 모델이 아닌 seq 2 seq 기반일때 batch가 앞에 오면, 병렬화에 대한 이점을 누리기 어렵다.

time seq가 없는 bert와 같은 모델은 layer 크기보다 batch 크기가 앞에 와도 상관없다. 하지만 동일하게 사용하는 것이 좋지 않을까, 라는 의견.
